{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "\n",
    "import nltk #Tokenizing\n",
    "import re #Preprocessing\n",
    "\n",
    "# Importing classes within the packages\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords #Don't consider stopwords in histogram\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify the directory where the transcripts are\n",
    "file_directory = 'data/transcripts'\n",
    " \n",
    "# We include those filenames ending with '.txt'.\n",
    "filename_pattern = '.+\\.txt'\n",
    "my_corpus = PlaintextCorpusReader(file_directory, filename_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences\n",
    "\n",
    "sentences = my_corpus.sents('19_08_09.txt')\n",
    "sent_list=[] # A list of all sentences in the transcript\n",
    "\n",
    "for key1 in sentences:\n",
    "    s=\"\"\n",
    "    for key2 in key1:\n",
    "        s=s+key2+\" \"\n",
    "    sent_list.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0:\n",
      " constitution\n",
      " ideology\n",
      " blacks\n",
      " basis\n",
      " trying\n",
      " malays\n",
      " black\n",
      " aspirations\n",
      " nation\n",
      " say\n",
      "\n",
      "Cluster 1:\n",
      " aspiration\n",
      " years\n",
      " failed\n",
      " evident\n",
      " ex\n",
      " exceptional\n",
      " explicitly\n",
      " extra\n",
      " fail\n",
      " false\n",
      "\n",
      "Cluster 2:\n",
      " field\n",
      " level\n",
      " going\n",
      " reach\n",
      " playing\n",
      " centuries\n",
      " decades\n",
      " trying\n",
      " everybody\n",
      " position\n",
      "\n",
      "Cluster 3:\n",
      " recognise\n",
      " starting\n",
      " base\n",
      " started\n",
      " foundations\n",
      " remind\n",
      " point\n",
      " earth\n",
      " thought\n",
      " fail\n",
      "\n",
      "Cluster 4:\n",
      " endowed\n",
      " hold\n",
      " truths\n",
      " evident\n",
      " created\n",
      " pursuit\n",
      " inalienable\n",
      " happiness\n",
      " creator\n",
      " certain\n"
     ]
    }
   ],
   "source": [
    "# Clustering- taking all sentences in the list\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sent_list)\n",
    "\n",
    "true_k = 5 # can change this value according to how many clusters are needed\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"\\nCluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting lemmatized words to do a different cluster analysis\n",
    "# Same procedure as NLP File\n",
    "\n",
    "# Reading the text file\n",
    "\n",
    "data=''\n",
    "with open('data/transcripts/19_08_09.txt','r') as file:\n",
    "    for line in file:\n",
    "        s = str(line).strip()\n",
    "        data = data + s + ' '\n",
    "        \n",
    "data = data.lower() # Contains all the data in the transcript as a string\n",
    "data_split = data.split(\" \")\n",
    "\n",
    "# Removing punctuations and only considering words\n",
    "\n",
    "pattern = r\"\\w+\"\n",
    "reg_split = re.findall(pattern,data) # A list of all words in the transcript \n",
    "\n",
    "# Tokenizing all the data\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data_tokens = tokenizer.tokenize(data)\n",
    "\n",
    "# Now we remove stopwords from the data\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for w in data_tokens:\n",
    "    if w not in stop_words:\n",
    "        clean_data.append(w)        \n",
    "\n",
    "# We stem the data using Porter Stemmer\n",
    "\n",
    "stemmed_data=[]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for w in clean_data:\n",
    "    stemmed_data.append(stemmer.stem(w))\n",
    "\n",
    "# Attach Parts of Speech to each word belonging to the clean data\n",
    "\n",
    "pos_data = pos_tag(clean_data)\n",
    "\n",
    "# We simplify the Parts of Speech tags\n",
    "# We want to convert NNS to n and VBD to v\n",
    "\n",
    "data_output=[]\n",
    "\n",
    "for w in pos_data:\n",
    "    data_output.append((w[0], w[1][0].lower()))\n",
    "\n",
    "# We use a Word Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_data=[]\n",
    "\n",
    "for w in clean_data: \n",
    "    lemmatized_data.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "# We combine the POS Tags and Lemmatized data\n",
    "\n",
    "lemma_output_data = []\n",
    "lemma_words=[]\n",
    "\n",
    "for w in data_output:\n",
    "    word = w[0]\n",
    "    pos_word = w[1]\n",
    "    lemma = word\n",
    "    if pos_word in ['a', 's', 'r', 'n', 'v']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos_word)\n",
    "        lemma_words.append(lemma)\n",
    "    \n",
    "    #return (word, pos_tag, lemma)\n",
    "    lemma_output_data.append((word,pos_word,lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0:\n",
      " malay\n",
      " constitution\n",
      " make\n",
      " treat\n",
      " aspiration\n",
      " position\n",
      " singapore\n",
      " try\n",
      " government\n",
      " read\n",
      "\n",
      "Cluster 1:\n",
      " way\n",
      " year\n",
      " fee\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      "\n",
      "Cluster 2:\n",
      " right\n",
      " year\n",
      " fee\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      "\n",
      "Cluster 3:\n",
      " black\n",
      " year\n",
      " fight\n",
      " government\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      "\n",
      "Cluster 4:\n",
      " place\n",
      " year\n",
      " field\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n"
     ]
    }
   ],
   "source": [
    "# Clustering- on the lemmatized words\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(lemma_words)\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"\\nCluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
