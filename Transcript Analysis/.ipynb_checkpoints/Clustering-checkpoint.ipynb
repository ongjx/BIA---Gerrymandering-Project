{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "\n",
    "import nltk #Tokenizing\n",
    "import re #Preprocessing\n",
    "\n",
    "# Importing classes within the packages\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords #Don't consider stopwords in histogram\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_02_19.txt', '01_06_18.txt', '01_09_11.txt', '01_10_15.txt', '02_02_11.txt', '03_08_11.txt', '08_07_11.txt', '08_08_11.txt', '11_10_18.txt', '12_09_18.txt', '13_11_15.txt', '14_08_11.txt', '14_11_18.txt', '15_11_18.txt', '16_03_18.txt', '16_05_18.txt', '16_08_11.txt', '16_11_18.txt', '17_02_15.txt', '18_05_11.txt', '19_08_09.txt', '19_08_18.txt', '21_05_11.txt', '21_10_11.txt', '23_08_15.txt', '24_04_18.txt', '24_09_15.txt', '25_01_19.txt', '26_04_18.txt', '26_10_15.txt', '27_04_18.txt', '27_09_15.txt', '28_04_18.txt', '28_09_15.txt', '29_03_15.txt', '29_08_11.txt', '29_09_15.txt', '31_08_11.txt', '31_10_11.txt', '31_10_15.txt', 'Subtitles1.txt', 'Subtitles10.txt', 'Subtitles11.txt', 'Subtitles12.txt', 'Subtitles13.txt', 'Subtitles14.txt', 'Subtitles15.txt', 'Subtitles16.txt', 'Subtitles17.txt', 'Subtitles18.txt', 'Subtitles19.txt', 'Subtitles2.txt', 'Subtitles20.txt', 'Subtitles21.txt', 'Subtitles22.txt', 'Subtitles23.txt', 'Subtitles24.txt', 'Subtitles25.txt', 'Subtitles26.txt', 'Subtitles27.txt', 'Subtitles28.txt', 'Subtitles3.txt', 'Subtitles4.txt', 'Subtitles5.txt', 'Subtitles6.txt', 'Subtitles7.txt', 'Subtitles8.txt', 'Subtitles9.txt']\n"
     ]
    }
   ],
   "source": [
    "# We specify the directory where the transcripts are\n",
    "file_directory = 'transcripts'\n",
    " \n",
    "# We include those filenames ending with '.txt'.\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus = PlaintextCorpusReader('transcripts', '.+\\.txt', encoding='Latin-1')\n",
    "fids = corpus.fileids()\n",
    "print(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x96 in position 89: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b834c2c91577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlist_of_words\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msentence_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-b834c2c91577>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlist_of_words\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msentence_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[0;32m    298\u001b[0m                 \u001b[1;34m'block reader %s() should return list or tuple.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m_read_sent_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_sent_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0msents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_para_block_reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             sents.extend([self._word_tokenizer.tokenize(sent)\n\u001b[0;32m    129\u001b[0m                           for sent in self._sent_tokenizer.tokenize(para)])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36mread_blankline_block\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m         \u001b[1;31m# End of file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mstartpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m             \u001b[0mnew_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m             \u001b[1;31m# If we're at a '\\r', then read one extra character, since\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[1;31m# Decode the bytes into unicode characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m         \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_decoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_incr_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[1;31m# If we got bytes but couldn't decode any, then read further.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_incr_decode\u001b[1;34m(self, bytes)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m                 \u001b[1;31m# If the exception occurs at the end of the string,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\utf_8.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x96 in position 89: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Extract sentences\n",
    "\n",
    "sentence_list=[] # a nested list of all sentences in all the files\n",
    "\n",
    "for filename in fids:\n",
    "    dummy = my_corpus.sents(filename)\n",
    "    sentences = [\" \".join(list_of_words) for list_of_words in dummy]\n",
    "    print(sentences)\n",
    "    sentence_list.append(sentences)\n",
    "\n",
    "print(sentence_list)\n",
    "\n",
    "sent_list=[]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for word1 in sentence:\n",
    "        s=\"\"\n",
    "        for word2 in key1:\n",
    "            s=s+key2+\" \"\n",
    "        sent_list.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering- taking all sentences in the list\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sent_list)\n",
    "\n",
    "true_k = 5 # can change this value according to how many clusters are needed\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"\\nCluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting lemmatized words to do a different cluster analysis\n",
    "# Same procedure as NLP File\n",
    "\n",
    "# Reading the text file\n",
    "\n",
    "data=''\n",
    "with open('transcripts/19_08_09.txt','r') as file:\n",
    "    for line in file:\n",
    "        s = str(line).strip()\n",
    "        data = data + s + ' '\n",
    "        \n",
    "data = data.lower() # Contains all the data in the transcript as a string\n",
    "data_split = data.split(\" \")\n",
    "\n",
    "# Removing punctuations and only considering words\n",
    "\n",
    "pattern = r\"\\w+\"\n",
    "reg_split = re.findall(pattern,data) # A list of all words in the transcript \n",
    "\n",
    "# Tokenizing all the data\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data_tokens = tokenizer.tokenize(data)\n",
    "\n",
    "# Now we remove stopwords from the data\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "for w in data_tokens:\n",
    "    if w not in stop_words:\n",
    "        clean_data.append(w)        \n",
    "\n",
    "# We stem the data using Porter Stemmer\n",
    "\n",
    "stemmed_data=[]\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for w in clean_data:\n",
    "    stemmed_data.append(stemmer.stem(w))\n",
    "\n",
    "# Attach Parts of Speech to each word belonging to the clean data\n",
    "\n",
    "pos_data = pos_tag(clean_data)\n",
    "\n",
    "# We simplify the Parts of Speech tags\n",
    "# We want to convert NNS to n and VBD to v\n",
    "\n",
    "data_output=[]\n",
    "\n",
    "for w in pos_data:\n",
    "    data_output.append((w[0], w[1][0].lower()))\n",
    "\n",
    "# We use a Word Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_data=[]\n",
    "\n",
    "for w in clean_data: \n",
    "    lemmatized_data.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "# We combine the POS Tags and Lemmatized data\n",
    "\n",
    "lemma_output_data = []\n",
    "lemma_words=[]\n",
    "\n",
    "for w in data_output:\n",
    "    word = w[0]\n",
    "    pos_word = w[1]\n",
    "    lemma = word\n",
    "    if pos_word in ['a', 's', 'r', 'n', 'v']:\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos_word)\n",
    "        lemma_words.append(lemma)\n",
    "    \n",
    "    #return (word, pos_tag, lemma)\n",
    "    lemma_output_data.append((word,pos_word,lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0:\n",
      " constitution\n",
      " right\n",
      " make\n",
      " treat\n",
      " black\n",
      " aspiration\n",
      " singapore\n",
      " position\n",
      " try\n",
      " government\n",
      "\n",
      "Cluster 1:\n",
      " reality\n",
      " year\n",
      " fee\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      "\n",
      "Cluster 2:\n",
      " malay\n",
      " year\n",
      " fee\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      "\n",
      "Cluster 3:\n",
      " university\n",
      " year\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      " fight\n",
      "\n",
      "Cluster 4:\n",
      " intend\n",
      " year\n",
      " govern\n",
      " gi\n",
      " ghetto\n",
      " fund\n",
      " foundation\n",
      " forward\n",
      " flaw\n",
      " fight\n"
     ]
    }
   ],
   "source": [
    "# Clustering- on the lemmatized words\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(lemma_words)\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"\\nCluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
